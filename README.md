# English to Japanese Translator by [pytorch](https://pytorch.org/) ðŸ™Š ([Transformer](https://arxiv.org/abs/1706.03762) from scratch)

## Overview

- English to Japanese translator by [Pytorch](https://pytorch.org/).
- The neural network architecture is [Transformer](https://arxiv.org/abs/1706.03762).
- The layers for Transfomer are implemented from scratch by pytorch. (you can find them under [layers/transformer/](https://github.com/YadaYuki/en_ja_translator_pytorch/tree/master/layers/transformer))
- Parallel corpus(dataset) is [kftt](http://www.phontron.com/kftt/index-ja.html).

## Transformer

![image](https://user-images.githubusercontent.com/57289763/159227403-edf771bf-e639-48f7-befe-763471e646da.png)

- Transformer is a neural network model proposed in the paper â€˜[Attention Is All You Need](https://arxiv.org/abs/1706.03762)â€™

- As the paper's title said, transformer is a model based on Attention mechanism. Transformer does not use recursive calculation when training like RNN,LSTM
- Many of the models that have achieved high accuracy in various tasks in the NLP domain in recent years, such as BERT, GPT-3, and XLNet, have a Transformer-based structure.

## Requirements

- [poetry](https://python-poetry.org/) 1.0.10+
- [python](https://www.python.org/) 3.8+

## Setup

Install dependencies & create a virtual environment in project by running:

```
$ poetry install
```

set PYTHONPATH

```
export PYTHONPATH="$(pwd)"
```

Download & unzip parallel corpus([kftt](http://www.phontron.com/kftt/index-ja.html)) by running:

```
$ poetry run python ./utils/download.py
```

## Directories

The directory structure is as below.

```
.
â”œâ”€â”€ const
â”‚Â Â  â””â”€â”€ path.py
â”œâ”€â”€ corpus
â”‚Â Â  â””â”€â”€ kftt-data-1.0
â”œâ”€â”€ figure
â”œâ”€â”€ layers
â”‚Â Â  â””â”€â”€ transformer
â”‚Â Â      â”œâ”€â”€ Embedding.py
â”‚Â Â      â”œâ”€â”€ FFN.py
â”‚Â Â      â”œâ”€â”€ MultiHeadAttention.py
â”‚Â Â      â”œâ”€â”€ PositionalEncoding.py
â”‚Â Â      â”œâ”€â”€ ScaledDotProductAttention.py
â”‚Â Â      â”œâ”€â”€ TransformerDecoder.py
â”‚Â Â      â””â”€â”€ TransformerEncoder.py
â”œâ”€â”€ models
â”‚Â Â  â”œâ”€â”€ Transformer.py
â”‚Â Â  â””â”€â”€ __init__.py
â”œâ”€â”€ mypy.ini
â”œâ”€â”€ pickles
â”‚Â Â  â””â”€â”€ nn/
â”œâ”€â”€ poetry.lock
â”œâ”€â”€ poetry.toml
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ tests
â”‚Â Â  â”œâ”€â”€ conftest.py
â”‚Â Â  â”œâ”€â”€ layers/
â”‚Â Â  â”œâ”€â”€ models/
â”‚Â Â  â””â”€â”€ utils/
â”œâ”€â”€ train.py
â””â”€â”€ utils
    â”œâ”€â”€ dataset/
    â”œâ”€â”€ download.py
    â”œâ”€â”€ evaluation/
    â””â”€â”€ text/
```

## How to run

You can train model by running:

```
$ poetry run python train.py

epoch: 1
--------------------Train--------------------

train loss: 10.104473114013672, bleu score: 0.0,iter: 1/4403

train loss: 9.551202774047852, bleu score: 0.0,iter: 2/4403

train loss: 8.950608253479004, bleu score: 0.0,iter: 3/4403

train loss: 8.688143730163574, bleu score: 0.0,iter: 4/4403

train loss: 8.4220552444458, bleu score: 0.0,iter: 5/4403

train loss: 8.243291854858398, bleu score: 0.0,iter: 6/4403

train loss: 8.187620162963867, bleu score: 0.0,iter: 7/4403

train loss: 7.6360859870910645, bleu score: 0.0,iter: 8/4403

....
```

- For each epoch, the model at that point is saved under pickles/nn/
- When the training is finished, loss.png is saved under figure/

## Reference

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

## Licence

[MIT](https://github.com/YadaYuki/en_ja_translator_pytorch/blob/master/LICENSE)
